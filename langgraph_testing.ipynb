{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anaqi_Amir/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-03-06 13:04:28.506613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/Anaqi_Amir/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, TypedDict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from api.src.models.dev.faiss_indexes import FlatIndex\n",
    "\n",
    "from api.src.nlp import NLSPipeline\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "\n",
    "# Load index\n",
    "index = FlatIndex(\"recipe_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutritionFeatures(BaseModel):\n",
    "    user_ingredients: List[str] = Field(default_factory=list)\n",
    "    allergens: List[str] = Field(\n",
    "        default_factory=list,\n",
    "    )\n",
    "    calories: Optional[Tuple[float, float]] = (None, None)\n",
    "    total_fat: Optional[Tuple[float, float]] = (None, None)\n",
    "    saturated_fat: Optional[Tuple[float, float]] = (None, None)\n",
    "    carbs: Optional[Tuple[float, float]] = (None, None)\n",
    "    sugar: Optional[Tuple[float, float]] = (None, None)\n",
    "    sodium: Optional[Tuple[float, float]] = (None, None)\n",
    "    protein: Optional[Tuple[float, float]] = (None, None)\n",
    "\n",
    "\n",
    "def extract_nutritional_features(user_input: str) -> NutritionFeatures:\n",
    "    \"\"\"\n",
    "    Extract nutritional features from a user's natural language input.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser using the Pydantic model.\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NutritionFeatures)\n",
    "\n",
    "    # Retrieve format instructions that tell the LLM how to output the JSON.\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create a system message to set the context.\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an assistant that extracts nutritional features from user input, \"\n",
    "        \"including recognized allergens from the following categories: tree nuts, peanut, milk, \"\n",
    "        \"wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites.\"\n",
    "    )\n",
    "\n",
    "    # Create a human message prompt template that includes extraction instructions,\n",
    "    # formatting instructions, and explicit allergen categorization.\n",
    "    human_template = (\n",
    "        \"Extract the following nutritional features from the user's input. \"\n",
    "        \"For numerical fields, return a reasonable range of number (or null if unspecified) that isn't too strict unless specified. \"\n",
    "        \"For calories, return a generous range of numbers that are both above and below a specified number.\"\n",
    "        \"For list fields, return a JSON array of strings. \"\n",
    "        \"Ensure that allergens are only taken from the following categories: \"\n",
    "        \"tree nuts, peanut, milk, wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites. \"\n",
    "        \"If the user does not specify any ingredients, feel free to be creative to add neccessary ingredients.\"\n",
    "        \"{format_instructions}\\n\\n\"\n",
    "        \"User input: {user_input}\"\n",
    "    )\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    # Build the full chat prompt.\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "    # Initialize the ChatOpenAI model with a low temperature for deterministic output.\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.25, max_tokens=500)\n",
    "\n",
    "    # Create the LLM chain with the prompt.\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "    # Run the chain by providing the user input and format instructions.\n",
    "    response = chain.run(\n",
    "        {\"user_input\": user_input, \"format_instructions\": format_instructions}\n",
    "    )\n",
    "\n",
    "    # Parse the LLM's response using our structured output parser.\n",
    "    features = output_parser.parse(response)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In chatbot: Istanbul\n",
      "In get_info: Istanbul is the largest city in Turkey and serves as the country's economic, cultural, and historic center. It is located in northwestern Turkey, straddling the Bosphorus Strait that separates Europe and Asia. Istanbul has a rich history that dates back thousands of years, with landmarks such as the Hagia Sophia, Blue Mosque, and Topkapi Palace. The city is known for its vibrant markets, delicious cuisine, and bustling street life. Istanbul is also a major tourist destination, attracting\n",
      "In final_answer: Istanbul is the largest city in Turkey, located in northwestern Turkey and serving as the country's economic, cultural, and historic center. It has a rich history dating back thousands of years, with landmarks such as the Hagia Sophia, Blue Mosque, and Topkapi Palace. The city is known for its vibrant markets, delicious cuisine, and bustling street life.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Istanbul is the largest city in Turkey, located in northwestern Turkey and serving as the country's economic, cultural, and historic center. It has a rich history dating back thousands of years, with landmarks such as the Hagia Sophia, Blue Mosque, and Topkapi Palace. The city is known for its vibrant markets, delicious cuisine, and bustling street life.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
    "AgentState = {}\n",
    "AgentState[\"messages\"] = []\n",
    "\n",
    "\n",
    "def chatbot(state):\n",
    "    messages = state[\"messages\"]\n",
    "    user_input = messages[-1]\n",
    "    complete_query = (\n",
    "        \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \"\n",
    "        + user_input\n",
    "    )\n",
    "    response = llm.invoke(complete_query)\n",
    "    print(\"In chatbot: \" + response.content)\n",
    "    state[\"messages\"].append(response.content)\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_info(state):\n",
    "    messages = state[\"messages\"]\n",
    "    chatbot_response = messages[-1]\n",
    "    complete_query = (\n",
    "        \"Your task is to find basic information of the city provided by the user. \\\n",
    "        Following is the user query: \"\n",
    "        + chatbot_response\n",
    "    )\n",
    "    response = llm.invoke(complete_query)\n",
    "    print(\"In get_info: \" + response.content)\n",
    "    state[\"messages\"].append(response.content)\n",
    "    return state\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    messages = state[\"messages\"]\n",
    "    initial_user_query = messages[0]\n",
    "    city_info = messages[-1]\n",
    "    complete_query = (\n",
    "        \"Your task if to provide a concise answer based on the user query and the available info\\\n",
    "        User query: \"\n",
    "        + initial_user_query\n",
    "        + \"Information: \"\n",
    "        + city_info\n",
    "    )\n",
    "    response = llm.invoke(complete_query)\n",
    "    print(\"In final_answer: \" + response.content)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "from langgraph.graph import Graph\n",
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_node(\"get_info\", get_info)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.add_edge(\"chatbot\", \"get_info\")\n",
    "workflow.add_edge(\"get_info\", \"final_answer\")\n",
    "\n",
    "workflow.set_entry_point(\"chatbot\")\n",
    "workflow.set_finish_point(\"final_answer\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "inputs = {\"messages\": [\"how are you\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## James Briggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "class NutritionFeatures(BaseModel):\n",
    "    user_ingredients: List[str] = Field(default_factory=list)\n",
    "    allergens: List[str] = Field(default_factory=list,)\n",
    "    calories: Optional[Tuple[float, float]] = ( None,None)\n",
    "    total_fat: Optional[Tuple[float, float]] = (None,None)\n",
    "    saturated_fat: Optional[Tuple[float, float]] = (None,None)\n",
    "    carbs: Optional[Tuple[float, float]] = (None,None)\n",
    "    sugar: Optional[Tuple[float, float]] = (None,None)\n",
    "    sodium: Optional[Tuple[float, float]] = (None, None)\n",
    "    protein: Optional[Tuple[float, float]] = (None,None)\n",
    "\n",
    "def extract_nutritional_features(user_input: str) -> NutritionFeatures:\n",
    "    \"\"\"\n",
    "    Extract nutritional features from a user's natural language input.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser using the Pydantic model.\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NutritionFeatures)\n",
    "\n",
    "    # Retrieve format instructions that tell the LLM how to output the JSON.\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create a system message to set the context.\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an assistant that extracts nutritional features from user input, \"\n",
    "        \"including recognized allergens from the following categories: tree nuts, peanut, milk, \"\n",
    "        \"wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites.\"\n",
    "    )\n",
    "\n",
    "    # Create a human message prompt template that includes extraction instructions,\n",
    "    # formatting instructions, and explicit allergen categorization.\n",
    "    human_template = (\n",
    "        \"Extract the following nutritional features from the user's input. \"\n",
    "        \"For numerical fields, return a reasonable range of number (or null if unspecified) that isn't too strict unless specified. \"\n",
    "        \"For calories, return a generous range of numbers that are both above and below a specified number.\"\n",
    "        \"For list fields, return a JSON array of strings. \"\n",
    "        \"Ensure that allergens are only taken from the following categories: \"\n",
    "        \"tree nuts, peanut, milk, wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites. \"\n",
    "        \"If the user does not specify any ingredients, feel free to be creative to add neccessary ingredients.\"\n",
    "        \"{format_instructions}\\n\\n\"\n",
    "        \"User input: {user_input}\"\n",
    "    )\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    # Build the full chat prompt.\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "    # Initialize the ChatOpenAI model with a low temperature for deterministic output.\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.25, max_tokens=500)\n",
    "\n",
    "    # Create the LLM chain with the prompt.\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "    # Run the chain by providing the user input and format instructions.\n",
    "    response = chain.run(\n",
    "        {\"user_input\": user_input, \"format_instructions\": format_instructions}\n",
    "    )\n",
    "\n",
    "    # Parse the LLM's response using our structured output parser.\n",
    "    features = output_parser.parse(response)\n",
    "    return features\n",
    "\n",
    "@tool(\"recipe_recommender\")\n",
    "def recipe_recommender(state: AgentState):\n",
    "    \"\"\"Takes in user input, extracts relevant features, and output recommended recipes from database\"\"\"\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "    extracted_features = extract_nutritional_features(last_message)\n",
    "\n",
    "    recs = index.recommend_recipes(\n",
    "        user_ingredients=extracted_features.user_ingredients,\n",
    "        calories=extracted_features.calories,\n",
    "        total_fat=extracted_features.total_fat,\n",
    "        protein=extracted_features.protein,\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [' '.join(recs)]}\n",
    "\n",
    "@tool(\"final_answer\")\n",
    "def final_answer()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are a chatbot for a recipe recommender appplication. Your tasks is to communicate with the user and try to figure out what type of recipes they want\n",
    "    in as little time as possible. Try your best to ensure that the user provide at least a few ingredients that they like for better recommendations.\n",
    "    If they insist on not giving you any ingredients, then feel free to be creative.\n",
    "\n",
    "    Be aware of nutritional restrictions that the user's are providing as well.\n",
    "\n",
    "    Most importantly, try to ask the users if they are allergic to anything.\n",
    "\n",
    "    Finally, if the user is not providing you information of the task at hand, be friendly and prompt them to include the relevant details.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/n8ssmgtj6nd825347vg7wz240000gn/T/ipykernel_66469/1561615205.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m tools\u001b[38;5;241m=\u001b[39m[recipe_recommender]\n\u001b[1;32m     11\u001b[0m oracle \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     12\u001b[0m     {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43many\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1189\u001b[0m, in \u001b[0;36mBaseChatModel.bind_tools\u001b[0;34m(self, tools, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1184\u001b[0m     tools: Sequence[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolCall, ToolMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=500)\n",
    "\n",
    "tools = [recipe_recommender]\n",
    "\n",
    "oracle = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input\": \"please give me a high protein meal that has chicken in it.\",\n",
    "    \"chat_history\": [],\n",
    "    \"intermediate_steps\": [],\n",
    "}\n",
    "out = oracle.invoke(inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI with Misbah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the output schema using a Pydantic model.\n",
    "# from typing import Sequence\n",
    "# import operator\n",
    "# from langchain_core.messages import BaseMessage\n",
    "\n",
    "# class NutritionFeatures(BaseModel):\n",
    "#     user_ingredients: List[str] = Field(default_factory=list)\n",
    "#     allergens: List[str] = Field(default_factory=list,)\n",
    "#     calories: Optional[Tuple[float, float]] = ( None,None)\n",
    "#     total_fat: Optional[Tuple[float, float]] = (None,None)\n",
    "#     saturated_fat: Optional[Tuple[float, float]] = (None,None)\n",
    "#     carbs: Optional[Tuple[float, float]] = (None,None)\n",
    "#     sugar: Optional[Tuple[float, float]] = (None,None)\n",
    "#     sodium: Optional[Tuple[float, float]] = (None, None)\n",
    "#     protein: Optional[Tuple[float, float]] = (None,None)\n",
    "\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "# def extract_nutritional_features(user_input: str) -> NutritionFeatures:\n",
    "#     \"\"\"\n",
    "#     Extract nutritional features from a user's natural language input.\n",
    "#     \"\"\"\n",
    "#     # Initialize the output parser using the Pydantic model.\n",
    "#     output_parser = PydanticOutputParser(pydantic_object=NutritionFeatures)\n",
    "\n",
    "#     # Retrieve format instructions that tell the LLM how to output the JSON.\n",
    "#     format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "#     # Create a system message to set the context.\n",
    "#     system_message = SystemMessagePromptTemplate.from_template(\n",
    "#         \"You are an assistant that extracts nutritional features from user input, \"\n",
    "#         \"including recognized allergens from the following categories: tree nuts, peanut, milk, \"\n",
    "#         \"wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites.\"\n",
    "#     )\n",
    "\n",
    "#     # Create a human message prompt template that includes extraction instructions,\n",
    "#     # formatting instructions, and explicit allergen categorization.\n",
    "#     human_template = (\n",
    "#         \"Extract the following nutritional features from the user's input. \"\n",
    "#         \"For numerical fields, return a reasonable range of number (or null if unspecified) that isn't too strict unless specified. \"\n",
    "#         \"For calories, return a generous range of numbers that are both above and below a specified number.\"\n",
    "#         \"For list fields, return a JSON array of strings. \"\n",
    "#         \"Ensure that allergens are only taken from the following categories: \"\n",
    "#         \"tree nuts, peanut, milk, wheat, soy, fish, shellfish, eggs, sesame, pollen, sulfites. \"\n",
    "#         \"If the user does not specify any ingredients, feel free to be creative to add neccessary ingredients.\"\n",
    "#         \"{format_instructions}\\n\\n\"\n",
    "#         \"User input: {user_input}\"\n",
    "#     )\n",
    "#     human_message = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "#     # Build the full chat prompt.\n",
    "#     chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "#     # Initialize the ChatOpenAI model with a low temperature for deterministic output.\n",
    "#     llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.25, max_tokens=500)\n",
    "\n",
    "#     # Create the LLM chain with the prompt.\n",
    "#     chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "#     # Run the chain by providing the user input and format instructions.\n",
    "#     response = chain.run(\n",
    "#         {\"user_input\": user_input, \"format_instructions\": format_instructions}\n",
    "#     )\n",
    "\n",
    "#     # Parse the LLM's response using our structured output parser.\n",
    "#     features = output_parser.parse(response)\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anaqi_Amir/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "# # from langgraph.prebuilt import ToolInvocation\n",
    "# # from langchain_core.messages import FunctionMessage\n",
    "# # from langgraph.prebuilt import ToolExecutor\n",
    "# # import json\n",
    "\n",
    "# from models.dev.faiss_indexes import FlatIndex\n",
    "\n",
    "# index = FlatIndex(\"recipe_embeddings.json\")\n",
    "\n",
    "# # tools = [extract_nutritional_features()]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5, max_tokens=200)\n",
    "# # functions = [convert_to_openai_function(t) for t in tools]\n",
    "# # model = llm.bind_functions(functions)\n",
    "\n",
    "# def chatbot(state: AgentState):\n",
    "#     messages = state['messages']\n",
    "#     response = llm.invoke(messages)\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# def recipe_recommender(state: AgentState):\n",
    "#     messages = state['messages']\n",
    "#     last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "#     extracted_features = extract_nutritional_features(last_message)\n",
    "\n",
    "#     recs = index.recommend_recipes(\n",
    "#         user_ingredients=extracted_features.user_ingredients,\n",
    "#         calories=extracted_features.calories,\n",
    "#         total_fat=extracted_features.total_fat,\n",
    "#         protein=extracted_features.protein,\n",
    "#     )\n",
    "\n",
    "#     return {\"messages\": [' '.join(recs)]}\n",
    "\n",
    "# graph = StateGraph(AgentState)\n",
    "\n",
    "# graph.add_node(\"chatbot\", chatbot)\n",
    "# graph.add_node(\"recommender\", recipe_recommender)\n",
    "\n",
    "# graph.add_edge(\"chatbot\", \"recommender\")\n",
    "\n",
    "# graph.set_entry_point(\"chatbot\")\n",
    "# graph.add_edge(\"recommender\", END)\n",
    "\n",
    "# app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'chatbot':\n",
      "---\n",
      "{'messages': [AIMessage(content=\"Sure! Here's a simple and delicious recipe for **Garlic Butter Chicken Thighs**. It’s easy to make and packed with flavor.\\n\\n### Garlic Butter Chicken Thighs\\n\\n#### Ingredients:\\n- 4 bone-in, skin-on chicken thighs (you can also use boneless, skinless if preferred)\\n- Salt and pepper, to taste\\n- 2 tablespoons olive oil\\n- 4 tablespoons unsalted butter\\n- 4 cloves garlic, minced\\n- 1 teaspoon dried thyme (or 1 tablespoon fresh thyme)\\n- 1 teaspoon paprika (optional)\\n- Juice of 1 lemon\\n- Fresh parsley, chopped (for garnish)\\n\\n#### Instructions:\\n\\n1. **Prep the Chicken:**\\n   - Pat the chicken thighs dry with paper towels. Season both sides generously with salt and pepper. If using paprika, sprinkle it on as well.\\n\\n2. **Sear the Chicken:**\\n   - In a large skillet, heat the olive oil over medium-high heat. Once hot\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 12, 'total_tokens': 212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'length', 'logprobs': None}, id='run-e45dc7e1-3294-4dcb-9e42-a025402efbaf-0')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/n8ssmgtj6nd825347vg7wz240000gn/T/ipykernel_29061/808559875.py:63: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'recommender':\n",
      "---\n",
      "{'messages': ['bacon roasted chicken with potatoes chicken danny style creamed chicken and corn soup chicken and eggplant  aubergine   pseudo fried creamy chicken taco soup']}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "# inputs = {\"messages\": [HumanMessage(content=\"Please give a chicken recipe\")]}\n",
    "# for output in app.stream(inputs):\n",
    "#     # stream() yields dictionaries with output keyed by node name\n",
    "#     for key, value in output.items():\n",
    "#         print(f\"Output from node '{key}':\")\n",
    "#         print(\"---\")\n",
    "#         print(value)\n",
    "#     print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5, max_tokens=200)\n",
    "\n",
    "# tools = [extract_nutritional_features]\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# # tool_node = ToolNode(tools=tools)\n",
    "# graph_builder.add_node(\"tools\", extract_nutritional_features)\n",
    "\n",
    "\n",
    "# # graph_builder.add_conditional_edges(\n",
    "# #     \"chatbot\",\n",
    "# #     tools_condition,\n",
    "# # )\n",
    "# # Any time a tool is called, we return to the chatbot to decide the next step\n",
    "# graph_builder.add_edge(\"chatbot\", \"tools\")\n",
    "# graph_builder.add_edge(\"tools\", END)\n",
    "# graph_builder.set_entry_point(\"chatbot\")\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stream_graph_updates(user_input: str):\n",
    "#     for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "#         for value in event.values():\n",
    "#             print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         user_input = input(\"User: \")\n",
    "#         if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "#             print(\"Goodbye!\")\n",
    "#             break\n",
    "\n",
    "#         stream_graph_updates(user_input)\n",
    "#     except:\n",
    "#         # fallback if input() is not available\n",
    "#         user_input = \"What do you know about LangGraph?\"\n",
    "#         print(\"User: \" + user_input)\n",
    "#         stream_graph_updates(user_input)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anaqi_Amir/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "from api.src.nlp.NLSPipeline import NutritionFeatures, extract_nutritional_features\n",
    "from api.src.models.dev.faiss_indexes import FlatIndex\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "\n",
    "# Load index\n",
    "index = FlatIndex(\"recipe_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed user input: 500 calories, low fat, high protein, no macadamia nuts, no whey protein, tomatoes, basil, no onions\n",
      "Response:\n",
      "{\n",
      "    \"user_ingredients\": [\"tomatoes\", \"basil\", \"onions\"],\n",
      "    \"allergens\": [\"tree nuts\", \"milk\"],\n",
      "    \"calories\": [500, 500],\n",
      "    \"total_fat\": [null, null],\n",
      "    \"saturated_fat\": [null, null],\n",
      "    \"carbs\": [null, null],\n",
      "    \"sugar\": [null, null],\n",
      "    \"sodium\": [null, null],\n",
      "    \"protein\": [null, null]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [ToolMessage(content='{\"messages\": [\"molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles molasses crinkles\"]}', name='recipe_recommender', tool_call_id='call_nOPd10K5Z0MsD7MgW0EPNLLc')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@tool(\"recipe_recommender\")\n",
    "def recipe_recommender(q: str) -> Dict:\n",
    "    \"\"\"Takes in user input, extracts relevant features, and output recommended recipes from database\"\"\"\n",
    "    print(f\"Parsed user input: {q}\")\n",
    "\n",
    "    extracted_features = extract_nutritional_features(q)\n",
    "\n",
    "    recs = index.recommend_recipes(\n",
    "        user_ingredients=extracted_features.user_ingredients,\n",
    "        allergens=extracted_features.allergens,\n",
    "        calories=extracted_features.calories,\n",
    "        total_fat=extracted_features.total_fat,\n",
    "        protein=extracted_features.protein,\n",
    "        saturated_fat=extracted_features.saturated_fat,\n",
    "        carbs=extracted_features.carbs,\n",
    "        sodium=extracted_features.sodium,\n",
    "        sugar=extracted_features.sugar,\n",
    "        top_n=10,\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [\" \".join(recs)]}\n",
    "\n",
    "\n",
    "@tool(\"final_answer\")\n",
    "def final_answer(recommendations: Dict) -> Dict:\n",
    "    \"\"\"Formats the query into a final answer\"\"\"\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "tools = [recipe_recommender, final_answer]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "def router(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"recommender\"\n",
    "    return END\n",
    "\n",
    "\n",
    "def chatbot(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.25, max_tokens=500).bind_tools(\n",
    "    tools\n",
    ")\n",
    "\n",
    "tool_node.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            llm.invoke(\n",
    "                \"Recommend me a recipe with around 500 calories, low fat (no more than 10g total fat), high protein, \\n\"\n",
    "                \"and please avoid macadamia nuts and whey protein. I love tomatoes and basil, but I don't like onions.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# graph = StateGraph(MessagesState)\n",
    "\n",
    "# graph.add_node(\"chatbot\", chatbot)\n",
    "# graph.add_node(\"recommender\", recipe_recommender)\n",
    "# graph.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "# graph.add_edge(START, \"chatbot\")\n",
    "# # graph.add_conditional_edges(\"chatbot\", router, [\"recommender\", END])\n",
    "# graph.add_edge(\"chatbot\", \"recommender\")\n",
    "# graph.add_edge(\"recommender\", \"final_answer\")\n",
    "# graph.add_edge(\"final_answer\", END)\n",
    "\n",
    "# app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Recommend me a recipe with around 500 calories, low fat (no more than 10g total fat), high protein, \n",
      "and please avoid macadamia nuts and whey protein. I love tomatoes and basil, but I don't like onions.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  recipe_recommender (call_vw4Bh8fN6Dt5vTls0Zqj1BnU)\n",
      " Call ID: call_vw4Bh8fN6Dt5vTls0Zqj1BnU\n",
      "  Args:\n",
      "    q: 500 calories, low fat, high protein, no macadamia nuts, no whey protein, tomatoes, basil, no onions\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for recipe_recommender\nq\n  Field required [type=missing, input_value={'messages': [HumanMessag...: 0, 'reasoning': 0}})]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecommend me a recipe with around 500 calories, low fat (no more than 10g total fat), high protein, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mand please avoid macadamia nuts and whey protein. I love tomatoes and basil, but I don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt like onions.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2024\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   2031\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langgraph/utils/runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/tools/base.py:509\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, ToolCall],\n\u001b[1;32m    505\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    507\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    508\u001b[0m     tool_input, kwargs \u001b[38;5;241m=\u001b[39m _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/tools/base.py:763\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    762\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    764\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    765\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/tools/base.py:727\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    726\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 727\u001b[0m tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    729\u001b[0m     tool_kwargs \u001b[38;5;241m=\u001b[39m tool_kwargs \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/tools/base.py:649\u001b[0m, in \u001b[0;36mBaseTool._to_args_and_kwargs\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m ):\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[0;32m--> 649\u001b[0m tool_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/langchain_core/tools/base.py:565\u001b[0m, in \u001b[0;36mBaseTool._parse_input\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    564\u001b[0m             tool_input[k] \u001b[38;5;241m=\u001b[39m tool_call_id\n\u001b[0;32m--> 565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minput_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     result_dict \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FlavorFusion/lib/python3.11/site-packages/pydantic/main.py:627\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    626\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for recipe_recommender\nq\n  Field required [type=missing, input_value={'messages': [HumanMessag...: 0, 'reasoning': 0}})]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
      "\u001b[0mDuring task with name 'recommender' and id 'fe19d203-0a16-f9ad-5296-5ae3d71a35e0'"
     ]
    }
   ],
   "source": [
    "for chunk in app.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Recommend me a recipe with around 500 calories, low fat (no more than 10g total fat), high protein, \\n\"\n",
    "                \"and please avoid macadamia nuts and whey protein. I love tomatoes and basil, but I don't like onions.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tools\n",
    "@tool\n",
    "def recommend_recipes(q: str) -> str:\n",
    "    \"\"\"Takes in user input, extracts relevant features, and output recommended recipes from database\"\"\"\n",
    "\n",
    "    print(f\"\\n Parsed user input: \\n {q} \\n\")\n",
    "\n",
    "    extracted_features = extract_nutritional_features(q)\n",
    "\n",
    "    print(f\"\\n Extracted features: \\n {extracted_features} \\n\")\n",
    "\n",
    "    recs = index.recommend_recipes(\n",
    "        user_ingredients=extracted_features.user_ingredients,\n",
    "        allergens=extracted_features.allergens,\n",
    "        calories=extracted_features.calories,\n",
    "        total_fat=extracted_features.total_fat,\n",
    "        protein=extracted_features.protein,\n",
    "        saturated_fat=extracted_features.saturated_fat,\n",
    "        carbs=extracted_features.carbs,\n",
    "        sodium=extracted_features.sodium,\n",
    "        sugar=extracted_features.sugar,\n",
    "        top_n=10,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n recs: \\n {recs} \\n\")\n",
    "\n",
    "    return \", \".join(recs)\n",
    "\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str) -> str:\n",
    "    \"\"\"Useful for providing the final answer to the user.\"\"\"\n",
    "    return answer\n",
    "\n",
    "\n",
    "tools = [recommend_recipes, final_answer]\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# Define the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a recipe recommendation program that takes in user inputs\n",
    "            and outputs a list of recipes. If a user does not provide enough information\n",
    "            on what recipes they want, you will keep on asking them about it until\n",
    "            you have enough information to get a good recommendation for the user.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_tool_calling_agent(llm=llm, prompt=prompt, tools=tools)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Define the LangGraph state\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Any]\n",
    "    ingredients: Optional[str]\n",
    "    nutrition: Optional[str]\n",
    "    recipes: Optional[str]\n",
    "\n",
    "\n",
    "# Define the nodes\n",
    "def agent_node(state: AgentState) -> Dict:\n",
    "    query = state[\"messages\"][-1]\n",
    "    chat_history = state[\"messages\"][:1]\n",
    "    print(f\"agent_node: query = {query}\")\n",
    "    print(f\"agent_node: chat_history = {chat_history}\")\n",
    "    result = agent_executor.invoke({\"query\": query, \"chat_history\": chat_history})\n",
    "    print(f\"agent_node: result = {result}\")\n",
    "    output = {\"messages\": state[\"messages\"] + [AIMessage(content=result[\"output\"])]}\n",
    "    print(f\"agent_node: output = {output}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def recommendation_node(state: AgentState):\n",
    "    print(f\"recommend_node: state = {state}\")\n",
    "    result = recommend_recipes.run(\n",
    "        ingredients=state[\"ingredients\"], nutrition=state[\"nutrition\"]\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"]\n",
    "        + [AIMessage(content=f\"Here are some recommended recipes: {result}\")],\n",
    "        \"recipes\": result,\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer_node(state: AgentState) -> Dict:\n",
    "    print(f\"final_answer_node: state = {state}\")\n",
    "    result = final_answer.run(answer=state[\"messages\"][-1])\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=result)]}\n",
    "\n",
    "\n",
    "# Define router\n",
    "def router(state: AgentState):\n",
    "    if \"recommend_recipes\" in state[\"messages\"][-1]:\n",
    "        return \"recommend\"\n",
    "    elif \"final_answer\" in state[\"messages\"][-1]:\n",
    "        return \"final\"\n",
    "    else:\n",
    "        return \"agent\"\n",
    "\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"recommendation_tool\", recommendation_node)\n",
    "workflow.add_node(\"final_answer_tool\", final_answer_node)\n",
    "\n",
    "# Set up the edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"recommend\": \"recommendation_tool\",\n",
    "        \"final\": \"final_answer_tool\",\n",
    "        \"agent\": \"agent\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"recommendation_tool\", \"agent\")\n",
    "workflow.add_edge(\"final_answer_tool\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Example usage\n",
    "# inputs = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(\n",
    "#             content=\"I'm looking for healthy recipes with chicken and low carbs.\"\n",
    "#         )\n",
    "#     ],\n",
    "#     \"ingredients\": \"chicken, garlic, honey\",\n",
    "#     \"nutrition\": \"High protein\",\n",
    "#     \"recipes\": \"Honey garlic chicken\",\n",
    "# }\n",
    "inputs = AgentState(\n",
    "    # messages=[\"I'm looking for healthy recipes with chicken and low carbs\"]\n",
    "    messages=[\n",
    "        \"Please recommend recipes that are healthy are have chicken. I want it to have low carbs too.\"\n",
    "    ]\n",
    ")\n",
    "result = app.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# inputs2 = {\"messages\": [HumanMessage(content=\"Hi, how are you?\")]}\n",
    "# result2 = app.invoke(inputs2)\n",
    "# print(result2)\n",
    "\n",
    "# inputs3 = {\"messages\": [HumanMessage(content=\"I need recipes with beef and high protein\")]}\n",
    "# result3 = app.invoke(inputs3)\n",
    "# print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The other James Briggs implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import operator\n",
    "\n",
    "\n",
    "# Define state\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def recommend_recipes(q: str) -> str:\n",
    "    \"\"\"Takes in user input, extracts relevant features, and output recommended recipes from database\"\"\"\n",
    "\n",
    "    print(f\"\\n Parsed user input: \\n {q} \\n\")\n",
    "\n",
    "    extracted_features = extract_nutritional_features(q)\n",
    "\n",
    "    print(f\"\\n Extracted features: \\n {extracted_features} \\n\")\n",
    "\n",
    "    recs = index.recommend_recipes(\n",
    "        user_ingredients=extracted_features.user_ingredients,\n",
    "        allergens=extracted_features.allergens,\n",
    "        calories=extracted_features.calories,\n",
    "        total_fat=extracted_features.total_fat,\n",
    "        protein=extracted_features.protein,\n",
    "        saturated_fat=extracted_features.saturated_fat,\n",
    "        carbs=extracted_features.carbs,\n",
    "        sodium=extracted_features.sodium,\n",
    "        sugar=extracted_features.sugar,\n",
    "        top_n=10,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n recs: \\n {recs} \\n\")\n",
    "\n",
    "    return \", \".join(recs)\n",
    "\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str) -> str:\n",
    "    \"\"\"Useful for providing the final answer to the user.\"\"\"\n",
    "    print(\"Hi im the final answer\")\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Define tool node\n",
    "tools = [recommend_recipes, final_answer]\n",
    "tool_node = ToolNode(tools)  # A single node that contains all the tools\n",
    "\n",
    "\n",
    "# Define llm\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Define prompt\n",
    "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
    "Given the user's query you must decide what to do with it based on the\n",
    "list of tools provided to you.\n",
    "\n",
    "You are also recipe recommendation program that takes in user inputs\n",
    "and outputs a list of recipes. If a user does not provide enough information\n",
    "on what recipes they want, you will keep on asking them about it until\n",
    "you have enough information to get a good recommendation for the user.\n",
    "\n",
    "If you see that a tool has been used (in the scratchpad) with a particular\n",
    "query, do NOT use that same tool with the same query again. Also, do NOT use\n",
    "any tool more than twice (ie, if the tool appears in the scratchpad twice, do\n",
    "not use it again).\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before\n",
    "providing the answer to the user. Once you have collected plenty of information\n",
    "to answer the user's question (stored in the scratchpad) use the final_answer\n",
    "tool.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"assistant\", \"scratchpad: {scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define scratchpad\n",
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    \"\"\"\n",
    "    Creates a scratchpad displaying the step-by-step input and output between Human and Agent Messsages.\n",
    "    \"\"\"\n",
    "    research_steps = []\n",
    "    for i, action in enumerate(intermediate_steps):\n",
    "        if action.log != \"TBD\":\n",
    "            # this was the ToolExecution\n",
    "            research_steps.append(\n",
    "                f\"Tool: {action.tool}, input: {action.tool_input}\\n\"\n",
    "                f\"Output: {action.log}\"\n",
    "            )\n",
    "    return \"\\n---\\n\".join(research_steps)\n",
    "\n",
    "\n",
    "# Define agent\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"scratchpad\": lambda x: create_scratchpad(\n",
    "            intermediate_steps=x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")\n",
    "\n",
    "# Test run\n",
    "inputs = {\n",
    "    \"input\": \"please give me a breakfast recipe with eggs but no milk\",\n",
    "    \"chat_history\": [],\n",
    "    \"intermediate_steps\": [],\n",
    "}\n",
    "out = agent.invoke(inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Agent node that decides which action to take and which tools to use.\n",
    "    \"\"\"\n",
    "    print(\"run_agent\")\n",
    "    print(f\"intermediate_steps: {state['intermediate_steps']}\")\n",
    "    out = agent.invoke(state)\n",
    "    tool_name = out.tool_calls[0][\"name\"]  # Get name of tool\n",
    "    tool_args = out.tool_calls[0][\"args\"]  # Get args that were passed to the tool\n",
    "    action_out = AgentAction(tool=tool_name, tool_input=tool_args, log=\"TBD\")\n",
    "    return {\"intermediate_steps\": [action_out]}\n",
    "\n",
    "\n",
    "def router(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Returns a string of the name of the tool that is to be used.\n",
    "    \"\"\"\n",
    "    # return the tool name to use\n",
    "    if isinstance(state[\"intermediate_steps\"], list):\n",
    "        return state[\"intermediate_steps\"][-1].tool\n",
    "    else:\n",
    "        # if we output bad format go to final answer\n",
    "        print(\"Router invalid format\")\n",
    "        return \"final_answer\"\n",
    "\n",
    "\n",
    "tool_str_to_func = {\n",
    "    \"recommend_recipes\": recommend_recipes,\n",
    "    \"final_answer\": final_answer,\n",
    "}\n",
    "\n",
    "\n",
    "def tools_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Tool node that invokes the chosen tool.\n",
    "    \"\"\"\n",
    "    # use this as helper function so we repeat less code\n",
    "    temp = state[\"intermediate_steps\"]\n",
    "    print(f\"tools_node intermediate steps: {temp}\")\n",
    "    tool_name = state[\"intermediate_steps\"][-1].tool\n",
    "    tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
    "    print(f\"{tool_name}.invoke(input={tool_args})\")\n",
    "    # run tool\n",
    "    out = tool_str_to_func[tool_name].invoke(input=tool_args)\n",
    "    action_out = AgentAction(tool=tool_name, tool_input=tool_args, log=str(out))\n",
    "    print(f\"action_out: {action_out}\")\n",
    "    return {\"intermediate_steps\": [action_out]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"recommend_recipes\", tools_node)\n",
    "graph.add_node(\"final_answer\", tools_node)\n",
    "\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    source=\"agent\",  # where in graph to start\n",
    "    path=router,  # function to determine which node is called\n",
    ")\n",
    "\n",
    "# create edges from each tool back to the oracle\n",
    "for tool_obj in tools:\n",
    "    if tool_obj.name != \"final_answer\":\n",
    "        graph.add_edge(tool_obj.name, \"agent\")\n",
    "\n",
    "# if anything goes to final answer, it must then move to END\n",
    "graph.add_edge(\"final_answer\", END)\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(runnable.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = runnable.invoke(\n",
    "    {\n",
    "        \"input\": \"i want a chicken rice recipe with calories between 0-2000, total fat between 0-1000, saturated fat between 0-1000, carbs between 0-1000, sugar between 0-200, sodium between 0-500mg, and protein between 0-200\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFinal output:\")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import pprint\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Currently, you're simply passing all HumanMessages into the chat history. You must update the ChatHistory with AIMesssages\n",
    "# 1) Figure out which stream_mode to use\n",
    "# 2) What elements to get the assistent_message\n",
    "def stream_graph_updates(user_input: str, state: AgentState):\n",
    "    state[\"chat_history\"].append(HumanMessage(content=user_input))\n",
    "    print(\"hi im here\")\n",
    "\n",
    "    for event in runnable.stream(\n",
    "        {\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": state[\"chat_history\"],\n",
    "            \"intermediate_steps\": state[\"intermediate_steps\"],\n",
    "        },\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(f\"event: {event}\")\n",
    "        assistant_message = event[\"chat_history\"][-1].content\n",
    "        print(\"Assistant:\", assistant_message)\n",
    "        state[\"chat_history\"].append(AIMessage(content=assistant_message))\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Initialize state if first interaction\n",
    "        if \"state\" not in locals():\n",
    "            state = {\"input\": user_input, \"chat_history\": [], \"intermediate_steps\": []}\n",
    "\n",
    "        print(state)\n",
    "\n",
    "        stream_graph_updates(user_input, state)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Currently, you're simply passing all HumanMessages into the chat history. You must update the ChatHistory with AIMesssages\n",
    "# 1) Figure out which stream_mode to use\n",
    "# 2) What elements to get the assistent_message\n",
    "def stream_graph_updates(user_input: str, state: AgentState) -> None:\n",
    "    # state[\"chat_history\"].append(HumanMessage(content=user_input))\n",
    "    print(f\"state before update: {state}\")\n",
    "    state[\"input\"] = user_input\n",
    "    state[\"chat_history\"].append(HumanMessage(content=user_input))\n",
    "    print(f\"state after user_input update: {state} \\n\")\n",
    "\n",
    "    events = runnable.stream(state, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        print(f\"event: {event}\")\n",
    "        print(f\"event['intermediate_steps]: {event['intermediate_steps']}\")\n",
    "        if len(event[\"intermediate_steps\"]) != 0:\n",
    "            if event[\"intermediate_steps\"][-1].tool == \"final_answer\":\n",
    "                print(\"here\")\n",
    "                output = event[\"intermediate_steps\"][-1].tool_input[\"answer\"]\n",
    "                state[\"chat_history\"].append(AIMessage(output))\n",
    "                print(f\"output: {output}\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Initialize state if first interaction\n",
    "        if \"state\" not in locals():\n",
    "            state = {\"input\": user_input, \"chat_history\": [], \"intermediate_steps\": []}\n",
    "\n",
    "        print(f\"user_input: {user_input}\")\n",
    "\n",
    "        stream_graph_updates(user_input, state)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlavorFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
